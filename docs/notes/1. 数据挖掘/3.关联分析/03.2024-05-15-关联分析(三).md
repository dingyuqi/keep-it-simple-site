---\rtitle: 关联分析-频繁项集的产生(2)\rcreateTime: 2024/05/15 15:33:05\rtags:\r  - 大数据\r  - 算法\r  - 关联分析\rpermalink: /dataMining/saueeew2/\r---\r\r在[上一节](/dataMining/ngr8k26m/)中我们简单介绍了先验原理以及该算法中的两个关键步骤: 候选集的产生和剪枝. \r\r本节将继续介绍支持度的计数以及计算复杂度, 这样关于频繁项集的产生的内容就全部介绍完毕.\r<!-- more -->\r\r## 支持度计数\r支持度计数的作用: 确定在候选剪枝步骤中保留下来的每个候选项集出现的频繁程度. 支持度计数出现在[伪代码](/dataMining/ngr8k26m/#apriori-pseudocode)的第6~11步实现.\r\r### 暴力方法\r将每个事务与所有的候选项集进行比较, 并更新包含在事务中的候选项集的支持度.\r\r这种方法的计算开销十分高, 尤其是事务和候选项集的数目都很大时.\r\r### 枚举方法\r枚举每个事务所包含的项集, 并利用其更新对应的候选项集的支持度.\r\r例如, 考虑事务$t$, 它包含5个项{1,2,3,5,6}. 对于想要考察的候选3-项集来说, 事务$t$中共有$C_5^3=10$个3-项集, 其中一定有部分项集是与候选3-项集重合的, 此时增加其支持度, 而不对应的子集则可以忽略.\r下图给出了一个系统地枚举事务$t$中所有3-项集的方法.\r\r![枚举方法](/screen_shot/enumeration-method.png)\r\r1. 将所有项按照字典顺序排列\r2. 针对于需要生成的k-项集, 选出k个最小项, 其他较大项跟随在后面. \r\r   例如, 给定 $t=\{1,2,3,5,6\}$的3-项集一定以项1、2、3生成的. **1** $\fbox{2 3 5 6}$ 表示的3-项集是: 从1开始, 后随两个取自集合$\{2,3,5,6\}$的项.\r3. 下一层的生成重复2的动作, 但是前缀的数量加一.\r\r\r### Hash树方法\r\r刚刚提到的枚举方法中我们知道了如何系统地枚举事务所包含的项集. 但是如果要进行支持度计数, 则还需要将候选项集与生成的k-项集进行匹配, 如果候选项集与枚举出来的项集匹配则支持度加一. 那么下面我们将解释如何使用Hash树来进行匹配这个动作, 这种存储候选集的结构可以大大减少比较的次数. 因为不在需要每个事务和每个候选集进行比较, 而是每个事务和Hash树中特定的候选集进行比较.\r\r\r\r我们仍然以书中的例子来看. 假设我们已经有了候选3-项集, 一共15个. 候选集如下:\r$\{1,4,5\},\{1,2,4\},\{4,5,7\},\{1,2,5\},\{4,5,8\},\{1,5,9\},\{1,3,6\},\{2,3,4\},$\r$\{5,6,7\},\{3,4,5\},\{3,5,6\},\{3,5,7\},\{6,8,9\},\{3,6,7\},\{3,6,8\}$\r\r#### 1. 建立Hash树\r::: details 前置准备\r1. 确定Hash函数. \r   \r   例如书中选用的函数: $h(p)=(p-1)\ mod\ 3$, 这会把与3取余之后结果相同的数值分到一个Hash分支中.\r```mermaid\r  graph TD\r    id1(Hash Function)\r    id2(1,4,7)\r    id3(2,5,8)\r    id4(3,6,9)\r    id1-->id2\r    id1-->id3\r    id1-->id4\r```\r2. 最大叶子节点数. \r  \r    如果一个叶子节点中所包含的数据个数超过最大叶子节点数, 则说明该叶子节点需要进一步的分裂. 此处将该数值设定为3.\r:::\r\r接下来我们需要开始建立这棵树.\r1. 对于项集$\{1,4,5\}$来说, 第一项为1, 根据hash函数, 我们应该将其放在左子树上.\r``` mermaid\rgraph TD\r  id1(root)\r  id2("{1,4,5}")\r  id3( )\r  id4( )\r  id1-->id2\r  id1-->id3\r  id1-->id4\r```\r\r\r2. 同理, $\{1,2,4\}$也放在左子树.\r``` mermaid\rgraph TD\r  id1(root)\r  id2("{1,4,5}\r       {1,2,4}")\r  id3( )\r  id4( )\r  id1-->id2\r  id1-->id3\r  id1-->id4\r```\r\r\r3. 我们继续看下一个候选项$\{4,5,7\}$. 根据第一项4可以判断其也应该放在左子树, 但是此时左子树的数据数量以及达到最大叶子节点数, 所以需要进行分裂. 我们根据这三个候选集的第二项进行hash处理, 可得到分裂的结果为:\r``` mermaid\rgraph TD\r  id1(root)\r  id2( )\r  id3( )\r  id4( )\r  id5("{1,4,5}")\r  id6("{1,2,4}\r       {4,5,7}")\r  id7( )\r  id1-->id2\r  id1-->id3\r  id1-->id4\r  id2-->id5\r  id2-->id6\r  id2-->id7\r```\r\r\r4. 再下一个候选项$\{1,2,5\}$的第一项是1应该在第一层的左子树, 第二项2应该在第二层的中间子树. 此时第二层的中间子树数据个数又一次超过最大叶子节点数, 所以该节点再一次分裂成:\r``` mermaid\rgraph TD\r  id1(root)\r  id2( )\r  id3( )\r  id4( )\r  id5("{1,4,5}")\r  id6( )\r  id7( )\r  id8("{1,2,4}\r       {4,5,7}")\r  id9("{1,2,5}")\r  id10( )\r  id1-->id2\r  id1-->id3\r  id1-->id4\r  id2-->id5\r  id2-->id6\r  id2-->id7\r  id6-->id8\r  id6-->id9\r  id6-->id10\r```\r\r\r5. 根据以上的规律, 不断将候选项加入到树中, 我们最终可以得到一棵完整的Hash树:\r``` mermaid\rgraph TD\r  id1(root)\r  id2( )\r  id3("{2,3,4}\r       {5,6,7}")\r  id4( )\r  id5("{1,4,5}")\r  id6( )\r  id7("{1,3,6}")\r  id8("{1,2,4}\r       {4,5,7}")\r  id9("{1,2,5}\r       {4,5,8}")\r  id10("{1,5,9}")\r  id11("{3,4,5}")\r  id12("{3,5,6}\r  {3,5,7},\r  {6,8,9}")\r  id13("{3,6,7}\r        {3,6,8}")\r  id1-->id2\r  id1-->id3\r  id1-->id4\r  id2-->id5\r  id2-->id6\r  id2-->id7\r  id6-->id8\r  id6-->id9\r  id6-->id10\r  id4-->id11\r  id4-->id12\r  id4-->id13\r```\r#### 2. 计算出一个事务可能的候选项集\r\r很显然我们在上一节[候选项集的产生](/dataMining/ngr8k26m/#候选项集的产生)中已经详细介绍过方法. \r\r#### 3. 使用Hash树进行计数\r我们仍然以事务$\{1,2,3,5,6\}$为例. \r\r所有3-项集中候选项的首项为1的对左子树进行比较, 而首项为2的则选择中间子树.首项为3的则选择右子树.接下来继续分析项集的第二项, 然后根据其数值选择下一层的子树分支.\r\r最后每个候选项必然走到一个叶子节点, 其仅仅需要与该节点上的数据进行比较从而计算支持度即可.\r\r## 计算复杂度\rApriori算法的计算复杂度受到如下因素的影响.\r### 支持度阈值\r降低支持度阈值通常会导致更多的项集被判断为频繁的, 这很不利于计算.\r### 项数(维度)\r项数主要影响的是内存的占用情况.而且随着维度的增加, 算法产生的候选项更多, 时间上也会增加.\r### 事务数\r由于Apriori算法反复扫描事务数据集, 因此运行时间随着事务数的增加而增加.\r### 事务的平均宽度\r对于稠密数据集, 事务的平均宽度可能很大. \r\r首先, 频繁项集的最大长度随着事务平均宽度的增加而增加. 这会导致候选项产生和支持度计数需要考察更多的项集. 其次, 随着事务宽度的增加, 事务中会包含更多的项集, 这将增加支持度计数时Hash树遍历的次数. \r\r\r\r